{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JDari\\Desktop\\STT\\StripTheText\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as npn\n",
    "from rouge import Rouge\n",
    "import rouge\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (C:/Users/JDari/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset xsum (C:/Users/JDari/.cache/huggingface/datasets/xsum/default/1.2.0/082863bf4754ee058a5b6f6525d0cb2b18eadb62c7b370b095d1364050a52b71)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set:  200\n"
     ]
    }
   ],
   "source": [
    "# Hier die größe der Test und Trainigsdaten Anpassen - auf dem Laptop geht das so nicht\n",
    "\n",
    "train_data = load_dataset('xsum', split='train[:5]')#50000\n",
    "print(\"Train Set: \",len(train_data))\n",
    "\n",
    "test_data= load_dataset('xsum', split='test[:200]')\n",
    "print(\"Test Set: \",len(test_data))\n",
    "\n",
    "train_article = train_data['document'][0]\n",
    "train_summary = train_data['summary'][0]\n",
    "test_article = test_data['document'][0]\n",
    "test_summary = test_data['summary'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)ve/main/spiece.model: 100%|██████████| 792k/792k [00:00<00:00, 6.80MB/s]\n",
      "c:\\Users\\JDari\\Desktop\\STT\\StripTheText\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\JDari\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<00:00, 1.21MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 892M/892M [01:41<00:00, 8.82MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=50000)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.21k/1.21k [00:00<?, ?B/s]\n",
      "Downloading model.safetensors: 100%|██████████| 242M/242M [00:33<00:00, 7.29MB/s] \n",
      "Downloading (…)neration_config.json: 100%|██████████| 147/147 [00:00<?, ?B/s] \n",
      "c:\\Users\\JDari\\Desktop\\STT\\StripTheText\\.venv\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [doc for doc in examples['document']]\n",
    "    targets = [summ for summ in examples['summary']]\n",
    "    \n",
    "    inputs_encoded = tokenizer.batch_encode_plus(inputs, truncation=True, padding='max_length', max_length=512)\n",
    "    targets_encoded = tokenizer.batch_encode_plus(targets, truncation=True, padding='max_length', max_length=512)\n",
    "    \n",
    "    input_ids = torch.tensor(inputs_encoded['input_ids'])\n",
    "    attention_mask = torch.tensor(inputs_encoded['attention_mask'])\n",
    "    labels = torch.tensor(targets_encoded['input_ids'])\n",
    "    \n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels}\n",
    "train_data_processed = train_data.map(preprocess_function, batched=True)\n",
    "test_data_processed = test_data.map(preprocess_function, batched=True)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 5\n",
    "batch_size = 2\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "#DataLoader\n",
    "train_dataloader = DataLoader(train_data_processed, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data_processed, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Traininge\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Average Loss = 29.1445\n",
      "Epoch 2: Average Loss = 21.0971\n",
      "Epoch 3: Average Loss = 17.1258\n",
      "Epoch 4: Average Loss = 13.1230\n",
      "Epoch 5: Average Loss = 9.7285\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_dataloader:\n",
    "        input_ids = torch.cat(batch['input_ids'], dim=0)\n",
    "        attention_mask = torch.cat(batch['attention_mask'], dim=0)\n",
    "        labels = torch.cat(batch['labels'], dim=0)\n",
    "\n",
    "        input_ids = input_ids.unsqueeze(0)\n",
    "        attention_mask = attention_mask.unsqueeze(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, decoder_input_ids=input_ids, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch + 1}: Average Loss = {average_loss:.4f}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\JDari\\Desktop\\STT\\StripTheText\\.venv\\Lib\\site-packages\\transformers\\generation\\utils.py:1353: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7.1850\n",
      "Average ROUGE-1: 0.0279\n",
      "Average ROUGE-2: 0.0008\n",
      "Average ROUGE-L: 0.0261\n"
     ]
    }
   ],
   "source": [
    "# Evaluationsschleife\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "total_rouge1 = 0.0\n",
    "total_rouge2 = 0.0\n",
    "total_rougeL = 0.0\n",
    "\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    input_ids = torch.cat(batch['input_ids'], dim=0)\n",
    "    attention_mask = torch.cat(batch['attention_mask'], dim=0)\n",
    "    labels = torch.cat(batch['labels'], dim=0)\n",
    "\n",
    "    input_ids = input_ids.unsqueeze(0)\n",
    "    attention_mask = attention_mask.unsqueeze(0)\n",
    "    labels = labels.unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "\n",
    "        predicted_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predicted_sentences = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "        reference_sentences = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        rouge_scores = rouge_scorer.get_scores(predicted_sentences, reference_sentences)\n",
    "        total_rouge1 += rouge_scores[0]['rouge-1']['f']\n",
    "        total_rouge2 += rouge_scores[0]['rouge-2']['f']\n",
    "        total_rougeL += rouge_scores[0]['rouge-l']['f']\n",
    "        \n",
    "    total_loss += loss.item()\n",
    "\n",
    "average_loss = total_loss / len(test_dataloader)\n",
    "average_rouge1 = total_rouge1 / len(test_dataloader)\n",
    "average_rouge2 = total_rouge2 / len(test_dataloader)\n",
    "average_rougeL = total_rougeL / len(test_dataloader)\n",
    "print(f\"Test Loss: {average_loss:.4f}\")\n",
    "print(f\"Average ROUGE-1: {average_rouge1:.4f}\")\n",
    "print(f\"Average ROUGE-2: {average_rouge2:.4f}\")\n",
    "print(f\"Average ROUGE-L: {average_rougeL:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base', model_max_length=50000)\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/spiece.model',\n",
       " 'tokenizer/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#speichern des modells \n",
    "modeloutput_dir = 'model/'\n",
    "tokenizeroutput_dir = \"tokenizer/\"\n",
    "model.save_pretrained(modeloutput_dir)\n",
    "tokenizer.save_pretrained(tokenizeroutput_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#laden des modells\n",
    "loadedmodel = T5ForConditionalGeneration.from_pretrained(modeloutput_dir)\n",
    "loadedtokenizer= T5Tokenizer.from_pretrained(tokenizeroutput_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: This example input will be generated with nlp methods like tokenization.\n"
     ]
    }
   ],
   "source": [
    "# Beispieltext\n",
    "example_text = \"This is an example input for summarization. It will be generated with nlp methodes like tokenization.\"\n",
    "input_ids = loadedtokenizer.encode(example_text, truncation=True, padding='longest', return_tensors='pt')\n",
    "output = loadedmodel.generate(input_ids=input_ids, max_length=100, num_beams=4, early_stopping=True)\n",
    "output_text = loadedtokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated Summary:\", output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spec_summ(input, rel_min):#input text als STR, rel_min ist der Prozentualteil auf den reduziert werden soll.(reduktion die mindestens erreicht werden soll)\n",
    "    summ_ready=False\n",
    "    counter=0\n",
    "    input_help=input\n",
    "    while counter<20 and summ_ready==False:\n",
    "        input_ids = loadedtokenizer.encode(input_help, truncation=True, padding='longest', return_tensors='pt')\n",
    "        output_pre = loadedmodel.generate(input_ids=input_ids, max_length=100, num_beams=4, early_stopping=True)\n",
    "        output_text = loadedtokenizer.decode(output_pre[0], skip_special_tokens=True)\n",
    "        if len(output_text)/len(input)<rel_min:\n",
    "            summ_ready=True\n",
    "        else:\n",
    "            input_help=output_text\n",
    "    rouge_scorer = Rouge()\n",
    "    rouge_scores = rouge_scorer.get_scores(output_text, input)\n",
    "\n",
    "    # Ausgabe der ROUGE-Werte\n",
    "    print(\"ROUGE-1 Score:\", rouge_scores[0]['rouge-1']['f'])\n",
    "    print(\"ROUGE-2 Score:\", rouge_scores[0]['rouge-2']['f'])\n",
    "    print(\"ROUGE-L Score:\", rouge_scores[0]['rouge-l']['f'])\n",
    "    if counter==20:\n",
    "        return \"!!!Gewünschte Kompression konnte nicht erreicht werden!!!\"+output_text\n",
    "    else:\n",
    "        return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3rd Duke of Norfolk (1415–1461), was a 15th-century English magnate who was the only son and heir of John Mowbray, 2nd Duke of Norfolk, and Katherine Neville. Mowbray was the son of John Mowbray, 2nd Duke of Norfolk, and Katherine Neville.\n"
     ]
    }
   ],
   "source": [
    "test=\"John Mowbray, 3rd Duke of Norfolk (1415–1461), was a fifteenth-century English magnate who was the only son and heir of John Mowbray, 2nd Duke of Norfolk, and Katherine Neville. As a minor he became a ward of King Henry VI and was placed under the protection of Humphrey, Duke of Gloucester, alongside whom Mowbray would later campaign in France. He led the defence of England's possessions in Normandy during the Hundred Years' War. He fought in Calais in 1436, and in 1437 and 1438 served as warden of the Eastern March on the Anglo-Scottish border. In the early 1430s he became the bitter rival of William de la Pole, Earl (later Duke) of Suffolk. In the early years of the Wars of the Roses in the 1450s he defended King Henry against two rebellions by Richard, Duke of York.\"\n",
    "dest_rel=0.8\n",
    "print(create_spec_summ(test,dest_rel))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
